{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x10a8a8770>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "preprocessed_text = [x.strip() for x in preprocessed_text if x.strip()]\n",
    "print((preprocessed_text[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed_text))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '--': 6,\n",
       " '.': 7,\n",
       " ':': 8,\n",
       " ';': 9,\n",
       " '?': 10,\n",
       " 'A': 11,\n",
       " 'Ah': 12,\n",
       " 'Among': 13,\n",
       " 'And': 14,\n",
       " 'Are': 15,\n",
       " 'Arrt': 16,\n",
       " 'As': 17,\n",
       " 'At': 18,\n",
       " 'Be': 19,\n",
       " 'Begin': 20,\n",
       " 'Burlington': 21,\n",
       " 'But': 22,\n",
       " 'By': 23,\n",
       " 'Carlo': 24,\n",
       " 'Chicago': 25,\n",
       " 'Claude': 26,\n",
       " 'Come': 27,\n",
       " 'Croft': 28,\n",
       " 'Destroyed': 29,\n",
       " 'Devonshire': 30,\n",
       " 'Don': 31,\n",
       " 'Dubarry': 32,\n",
       " 'Emperors': 33,\n",
       " 'Florence': 34,\n",
       " 'For': 35,\n",
       " 'Gallery': 36,\n",
       " 'Gideon': 37,\n",
       " 'Gisburn': 38,\n",
       " 'Gisburns': 39,\n",
       " 'Grafton': 40,\n",
       " 'Greek': 41,\n",
       " 'Grindle': 42,\n",
       " 'Grindles': 43,\n",
       " 'HAD': 44,\n",
       " 'Had': 45,\n",
       " 'Hang': 46,\n",
       " 'Has': 47,\n",
       " 'He': 48,\n",
       " 'Her': 49,\n",
       " 'Hermia': 50,\n",
       " 'His': 51,\n",
       " 'How': 52,\n",
       " 'I': 53,\n",
       " 'If': 54,\n",
       " 'In': 55,\n",
       " 'It': 56,\n",
       " 'Jack': 57,\n",
       " 'Jove': 58,\n",
       " 'Just': 59,\n",
       " 'Lord': 60,\n",
       " 'Made': 61,\n",
       " 'Miss': 62,\n",
       " 'Money': 63,\n",
       " 'Monte': 64,\n",
       " 'Moon-dancers': 65,\n",
       " 'Mr': 66,\n",
       " 'Mrs': 67,\n",
       " 'My': 68,\n",
       " 'Never': 69,\n",
       " 'No': 70,\n",
       " 'Now': 71,\n",
       " 'Nutley': 72,\n",
       " 'Of': 73,\n",
       " 'Oh': 74,\n",
       " 'On': 75,\n",
       " 'Once': 76,\n",
       " 'Only': 77,\n",
       " 'Or': 78,\n",
       " 'Perhaps': 79,\n",
       " 'Poor': 80,\n",
       " 'Professional': 81,\n",
       " 'Renaissance': 82,\n",
       " 'Rickham': 83,\n",
       " 'Riviera': 84,\n",
       " 'Rome': 85,\n",
       " 'Russian': 86,\n",
       " 'Sevres': 87,\n",
       " 'She': 88,\n",
       " 'Stroud': 89,\n",
       " 'Strouds': 90,\n",
       " 'Suddenly': 91,\n",
       " 'That': 92,\n",
       " 'The': 93,\n",
       " 'Then': 94,\n",
       " 'There': 95,\n",
       " 'They': 96,\n",
       " 'This': 97,\n",
       " 'Those': 98,\n",
       " 'Though': 99,\n",
       " 'Thwing': 100,\n",
       " 'Thwings': 101,\n",
       " 'To': 102,\n",
       " 'Usually': 103,\n",
       " 'Venetian': 104,\n",
       " 'Victor': 105,\n",
       " 'Was': 106,\n",
       " 'We': 107,\n",
       " 'Well': 108,\n",
       " 'What': 109,\n",
       " 'When': 110,\n",
       " 'Why': 111,\n",
       " 'Yes': 112,\n",
       " 'You': 113,\n",
       " '_': 114,\n",
       " 'a': 115,\n",
       " 'abdication': 116,\n",
       " 'able': 117,\n",
       " 'about': 118,\n",
       " 'above': 119,\n",
       " 'abruptly': 120,\n",
       " 'absolute': 121,\n",
       " 'absorbed': 122,\n",
       " 'absurdity': 123,\n",
       " 'academic': 124,\n",
       " 'accuse': 125,\n",
       " 'accustomed': 126,\n",
       " 'across': 127,\n",
       " 'activity': 128,\n",
       " 'add': 129,\n",
       " 'added': 130,\n",
       " 'admirers': 131,\n",
       " 'adopted': 132,\n",
       " 'adulation': 133,\n",
       " 'advance': 134,\n",
       " 'aesthetic': 135,\n",
       " 'affect': 136,\n",
       " 'afraid': 137,\n",
       " 'after': 138,\n",
       " 'afterward': 139,\n",
       " 'again': 140,\n",
       " 'ago': 141,\n",
       " 'ah': 142,\n",
       " 'air': 143,\n",
       " 'alive': 144,\n",
       " 'all': 145,\n",
       " 'almost': 146,\n",
       " 'alone': 147,\n",
       " 'along': 148,\n",
       " 'always': 149,\n",
       " 'am': 150,\n",
       " 'amazement': 151,\n",
       " 'amid': 152,\n",
       " 'among': 153,\n",
       " 'amplest': 154,\n",
       " 'amusing': 155,\n",
       " 'an': 156,\n",
       " 'and': 157,\n",
       " 'another': 158,\n",
       " 'answer': 159,\n",
       " 'answered': 160,\n",
       " 'any': 161,\n",
       " 'anything': 162,\n",
       " 'anywhere': 163,\n",
       " 'apparent': 164,\n",
       " 'apparently': 165,\n",
       " 'appearance': 166,\n",
       " 'appeared': 167,\n",
       " 'appointed': 168,\n",
       " 'are': 169,\n",
       " 'arm': 170,\n",
       " 'arm-chair': 171,\n",
       " 'arm-chairs': 172,\n",
       " 'arms': 173,\n",
       " 'art': 174,\n",
       " 'articles': 175,\n",
       " 'artist': 176,\n",
       " 'as': 177,\n",
       " 'aside': 178,\n",
       " 'asked': 179,\n",
       " 'at': 180,\n",
       " 'atmosphere': 181,\n",
       " 'atom': 182,\n",
       " 'attack': 183,\n",
       " 'attention': 184,\n",
       " 'attitude': 185,\n",
       " 'audacities': 186,\n",
       " 'away': 187,\n",
       " 'awful': 188,\n",
       " 'axioms': 189,\n",
       " 'azaleas': 190,\n",
       " 'back': 191,\n",
       " 'background': 192,\n",
       " 'balance': 193,\n",
       " 'balancing': 194,\n",
       " 'balustraded': 195,\n",
       " 'basking': 196,\n",
       " 'bath-rooms': 197,\n",
       " 'be': 198,\n",
       " 'beaming': 199,\n",
       " 'bean-stalk': 200,\n",
       " 'bear': 201,\n",
       " 'beard': 202,\n",
       " 'beauty': 203,\n",
       " 'became': 204,\n",
       " 'because': 205,\n",
       " 'becoming': 206,\n",
       " 'bed': 207,\n",
       " 'been': 208,\n",
       " 'before': 209,\n",
       " 'began': 210,\n",
       " 'begun': 211,\n",
       " 'behind': 212,\n",
       " 'being': 213,\n",
       " 'believed': 214,\n",
       " 'beneath': 215,\n",
       " 'bespoke': 216,\n",
       " 'better': 217,\n",
       " 'between': 218,\n",
       " 'big': 219,\n",
       " 'bits': 220,\n",
       " 'bitterness': 221,\n",
       " 'blocked': 222,\n",
       " 'born': 223,\n",
       " 'borne': 224,\n",
       " 'boudoir': 225,\n",
       " 'bravura': 226,\n",
       " 'break': 227,\n",
       " 'breaking': 228,\n",
       " 'breathing': 229,\n",
       " 'bric-a-brac': 230,\n",
       " 'briefly': 231,\n",
       " 'brings': 232,\n",
       " 'bronzes': 233,\n",
       " 'brought': 234,\n",
       " 'brown': 235,\n",
       " 'brush': 236,\n",
       " 'bull': 237,\n",
       " 'business': 238,\n",
       " 'but': 239,\n",
       " 'buying': 240,\n",
       " 'by': 241,\n",
       " 'called': 242,\n",
       " 'came': 243,\n",
       " 'can': 244,\n",
       " 'canvas': 245,\n",
       " 'canvases': 246,\n",
       " 'cards': 247,\n",
       " 'care': 248,\n",
       " 'career': 249,\n",
       " 'caught': 250,\n",
       " 'central': 251,\n",
       " 'chair': 252,\n",
       " 'chap': 253,\n",
       " 'characteristic': 254,\n",
       " 'charming': 255,\n",
       " 'cheap': 256,\n",
       " 'check': 257,\n",
       " 'cheeks': 258,\n",
       " 'chest': 259,\n",
       " 'chimney-piece': 260,\n",
       " 'chucked': 261,\n",
       " 'cigar': 262,\n",
       " 'cigarette': 263,\n",
       " 'cigars': 264,\n",
       " 'circulation': 265,\n",
       " 'circumstance': 266,\n",
       " 'circus-clown': 267,\n",
       " 'claimed': 268,\n",
       " 'clasping': 269,\n",
       " 'clear': 270,\n",
       " 'cleverer': 271,\n",
       " 'close': 272,\n",
       " 'clue': 273,\n",
       " 'coat': 274,\n",
       " 'collapsed': 275,\n",
       " 'colour': 276,\n",
       " 'come': 277,\n",
       " 'comfortable': 278,\n",
       " 'coming': 279,\n",
       " 'companion': 280,\n",
       " 'compared': 281,\n",
       " 'complex': 282,\n",
       " 'confident': 283,\n",
       " 'congesting': 284,\n",
       " 'conjugal': 285,\n",
       " 'constraint': 286,\n",
       " 'consummate': 287,\n",
       " 'contended': 288,\n",
       " 'continued': 289,\n",
       " 'corner': 290,\n",
       " 'corrected': 291,\n",
       " 'could': 292,\n",
       " 'couldn': 293,\n",
       " 'count': 294,\n",
       " 'countenance': 295,\n",
       " 'couple': 296,\n",
       " 'course': 297,\n",
       " 'covered': 298,\n",
       " 'craft': 299,\n",
       " 'cried': 300,\n",
       " 'crossed': 301,\n",
       " 'crowned': 302,\n",
       " 'crumbled': 303,\n",
       " 'cry': 304,\n",
       " 'cured': 305,\n",
       " 'curiosity': 306,\n",
       " 'curious': 307,\n",
       " 'current': 308,\n",
       " 'curtains': 309,\n",
       " 'd': 310,\n",
       " 'dabble': 311,\n",
       " 'damask': 312,\n",
       " 'dark': 313,\n",
       " 'dashed': 314,\n",
       " 'day': 315,\n",
       " 'days': 316,\n",
       " 'dead': 317,\n",
       " 'deadening': 318,\n",
       " 'dear': 319,\n",
       " 'deep': 320,\n",
       " 'deerhound': 321,\n",
       " 'degree': 322,\n",
       " 'delicate': 323,\n",
       " 'demand': 324,\n",
       " 'denied': 325,\n",
       " 'deploring': 326,\n",
       " 'deprecating': 327,\n",
       " 'deprecatingly': 328,\n",
       " 'desire': 329,\n",
       " 'destroyed': 330,\n",
       " 'destruction': 331,\n",
       " 'desultory': 332,\n",
       " 'detail': 333,\n",
       " 'diagnosis': 334,\n",
       " 'did': 335,\n",
       " 'didn': 336,\n",
       " 'died': 337,\n",
       " 'dim': 338,\n",
       " 'dimmest': 339,\n",
       " 'dingy': 340,\n",
       " 'dining-room': 341,\n",
       " 'disarming': 342,\n",
       " 'discovery': 343,\n",
       " 'discrimination': 344,\n",
       " 'discussion': 345,\n",
       " 'disdain': 346,\n",
       " 'disdained': 347,\n",
       " 'disease': 348,\n",
       " 'disguised': 349,\n",
       " 'display': 350,\n",
       " 'dissatisfied': 351,\n",
       " 'distinguished': 352,\n",
       " 'distract': 353,\n",
       " 'divert': 354,\n",
       " 'do': 355,\n",
       " 'doesn': 356,\n",
       " 'doing': 357,\n",
       " 'domestic': 358,\n",
       " 'don': 359,\n",
       " 'done': 360,\n",
       " 'donkey': 361,\n",
       " 'down': 362,\n",
       " 'dozen': 363,\n",
       " 'dragged': 364,\n",
       " 'drawing-room': 365,\n",
       " 'drawing-rooms': 366,\n",
       " 'drawn': 367,\n",
       " 'dress-closets': 368,\n",
       " 'drew': 369,\n",
       " 'dropped': 370,\n",
       " 'each': 371,\n",
       " 'earth': 372,\n",
       " 'ease': 373,\n",
       " 'easel': 374,\n",
       " 'easy': 375,\n",
       " 'echoed': 376,\n",
       " 'economy': 377,\n",
       " 'effect': 378,\n",
       " 'effects': 379,\n",
       " 'efforts': 380,\n",
       " 'egregious': 381,\n",
       " 'eighteenth-century': 382,\n",
       " 'elbow': 383,\n",
       " 'elegant': 384,\n",
       " 'else': 385,\n",
       " 'embarrassed': 386,\n",
       " 'enabled': 387,\n",
       " 'end': 388,\n",
       " 'endless': 389,\n",
       " 'enjoy': 390,\n",
       " 'enlightenment': 391,\n",
       " 'enough': 392,\n",
       " 'ensuing': 393,\n",
       " 'equally': 394,\n",
       " 'equanimity': 395,\n",
       " 'escape': 396,\n",
       " 'established': 397,\n",
       " 'etching': 398,\n",
       " 'even': 399,\n",
       " 'event': 400,\n",
       " 'ever': 401,\n",
       " 'everlasting': 402,\n",
       " 'every': 403,\n",
       " 'exasperated': 404,\n",
       " 'except': 405,\n",
       " 'excuse': 406,\n",
       " 'excusing': 407,\n",
       " 'existed': 408,\n",
       " 'expected': 409,\n",
       " 'exquisite': 410,\n",
       " 'exquisitely': 411,\n",
       " 'extenuation': 412,\n",
       " 'exterminating': 413,\n",
       " 'extracting': 414,\n",
       " 'eye': 415,\n",
       " 'eyebrows': 416,\n",
       " 'eyes': 417,\n",
       " 'face': 418,\n",
       " 'faces': 419,\n",
       " 'fact': 420,\n",
       " 'faded': 421,\n",
       " 'failed': 422,\n",
       " 'failure': 423,\n",
       " 'fair': 424,\n",
       " 'faith': 425,\n",
       " 'false': 426,\n",
       " 'familiar': 427,\n",
       " 'famille-verte': 428,\n",
       " 'fancy': 429,\n",
       " 'fashionable': 430,\n",
       " 'fate': 431,\n",
       " 'feather': 432,\n",
       " 'feet': 433,\n",
       " 'fell': 434,\n",
       " 'fellow': 435,\n",
       " 'felt': 436,\n",
       " 'few': 437,\n",
       " 'fewer': 438,\n",
       " 'finality': 439,\n",
       " 'find': 440,\n",
       " 'fingers': 441,\n",
       " 'first': 442,\n",
       " 'fit': 443,\n",
       " 'fitting': 444,\n",
       " 'five': 445,\n",
       " 'flash': 446,\n",
       " 'flashed': 447,\n",
       " 'florid': 448,\n",
       " 'flowers': 449,\n",
       " 'fluently': 450,\n",
       " 'flung': 451,\n",
       " 'follow': 452,\n",
       " 'followed': 453,\n",
       " 'fond': 454,\n",
       " 'footstep': 455,\n",
       " 'for': 456,\n",
       " 'forced': 457,\n",
       " 'forcing': 458,\n",
       " 'forehead': 459,\n",
       " 'foreign': 460,\n",
       " 'foreseen': 461,\n",
       " 'forgive': 462,\n",
       " 'forgotten': 463,\n",
       " 'form': 464,\n",
       " 'formed': 465,\n",
       " 'forming': 466,\n",
       " 'forward': 467,\n",
       " 'fostered': 468,\n",
       " 'found': 469,\n",
       " 'foundations': 470,\n",
       " 'fragment': 471,\n",
       " 'fragments': 472,\n",
       " 'frame': 473,\n",
       " 'frames': 474,\n",
       " 'frequently': 475,\n",
       " 'friend': 476,\n",
       " 'from': 477,\n",
       " 'full': 478,\n",
       " 'fullest': 479,\n",
       " 'furiously': 480,\n",
       " 'furrowed': 481,\n",
       " 'garlanded': 482,\n",
       " 'garlands': 483,\n",
       " 'gave': 484,\n",
       " 'genial': 485,\n",
       " 'genius': 486,\n",
       " 'gesture': 487,\n",
       " 'get': 488,\n",
       " 'getting': 489,\n",
       " 'give': 490,\n",
       " 'given': 491,\n",
       " 'glad': 492,\n",
       " 'glanced': 493,\n",
       " 'glimpse': 494,\n",
       " 'gloried': 495,\n",
       " 'glory': 496,\n",
       " 'go': 497,\n",
       " 'going': 498,\n",
       " 'gone': 499,\n",
       " 'good': 500,\n",
       " 'good-breeding': 501,\n",
       " 'good-humoured': 502,\n",
       " 'got': 503,\n",
       " 'grace': 504,\n",
       " 'gradually': 505,\n",
       " 'gray': 506,\n",
       " 'grayish': 507,\n",
       " 'great': 508,\n",
       " 'greatest': 509,\n",
       " 'greatness': 510,\n",
       " 'grew': 511,\n",
       " 'groping': 512,\n",
       " 'growing': 513,\n",
       " 'had': 514,\n",
       " 'hadn': 515,\n",
       " 'hair': 516,\n",
       " 'half': 517,\n",
       " 'half-light': 518,\n",
       " 'half-mechanically': 519,\n",
       " 'hall': 520,\n",
       " 'hand': 521,\n",
       " 'hands': 522,\n",
       " 'handsome': 523,\n",
       " 'hanging': 524,\n",
       " 'happen': 525,\n",
       " 'happened': 526,\n",
       " 'hard': 527,\n",
       " 'hardly': 528,\n",
       " 'has': 529,\n",
       " 'have': 530,\n",
       " 'haven': 531,\n",
       " 'having': 532,\n",
       " 'he': 533,\n",
       " 'head': 534,\n",
       " 'hear': 535,\n",
       " 'heard': 536,\n",
       " 'heart': 537,\n",
       " 'height': 538,\n",
       " 'her': 539,\n",
       " 'here': 540,\n",
       " 'hermit': 541,\n",
       " 'herself': 542,\n",
       " 'hesitations': 543,\n",
       " 'hide': 544,\n",
       " 'high': 545,\n",
       " 'him': 546,\n",
       " 'himself': 547,\n",
       " 'hint': 548,\n",
       " 'his': 549,\n",
       " 'history': 550,\n",
       " 'holding': 551,\n",
       " 'home': 552,\n",
       " 'honour': 553,\n",
       " 'hooded': 554,\n",
       " 'hostess': 555,\n",
       " 'hot-house': 556,\n",
       " 'hour': 557,\n",
       " 'hours': 558,\n",
       " 'house': 559,\n",
       " 'how': 560,\n",
       " 'hung': 561,\n",
       " 'husband': 562,\n",
       " 'idea': 563,\n",
       " 'idle': 564,\n",
       " 'idling': 565,\n",
       " 'if': 566,\n",
       " 'immediately': 567,\n",
       " 'in': 568,\n",
       " 'incense': 569,\n",
       " 'indifferent': 570,\n",
       " 'inevitable': 571,\n",
       " 'inevitably': 572,\n",
       " 'inflexible': 573,\n",
       " 'insensible': 574,\n",
       " 'insignificant': 575,\n",
       " 'instinctively': 576,\n",
       " 'instructive': 577,\n",
       " 'interesting': 578,\n",
       " 'into': 579,\n",
       " 'ironic': 580,\n",
       " 'irony': 581,\n",
       " 'irrelevance': 582,\n",
       " 'irrevocable': 583,\n",
       " 'is': 584,\n",
       " 'it': 585,\n",
       " 'its': 586,\n",
       " 'itself': 587,\n",
       " 'jardiniere': 588,\n",
       " 'jealousy': 589,\n",
       " 'just': 590,\n",
       " 'keep': 591,\n",
       " 'kept': 592,\n",
       " 'kind': 593,\n",
       " 'knees': 594,\n",
       " 'knew': 595,\n",
       " 'know': 596,\n",
       " 'known': 597,\n",
       " 'laid': 598,\n",
       " 'lair': 599,\n",
       " 'landing': 600,\n",
       " 'language': 601,\n",
       " 'last': 602,\n",
       " 'late': 603,\n",
       " 'later': 604,\n",
       " 'latter': 605,\n",
       " 'laugh': 606,\n",
       " 'laughed': 607,\n",
       " 'lay': 608,\n",
       " 'leading': 609,\n",
       " 'lean': 610,\n",
       " 'learned': 611,\n",
       " 'least': 612,\n",
       " 'leathery': 613,\n",
       " 'leave': 614,\n",
       " 'led': 615,\n",
       " 'left': 616,\n",
       " 'leisure': 617,\n",
       " 'lends': 618,\n",
       " 'lent': 619,\n",
       " 'let': 620,\n",
       " 'lies': 621,\n",
       " 'life': 622,\n",
       " 'life-likeness': 623,\n",
       " 'lift': 624,\n",
       " 'lifted': 625,\n",
       " 'light': 626,\n",
       " 'lightly': 627,\n",
       " 'like': 628,\n",
       " 'liked': 629,\n",
       " 'line': 630,\n",
       " 'lines': 631,\n",
       " 'lingered': 632,\n",
       " 'lips': 633,\n",
       " 'lit': 634,\n",
       " 'little': 635,\n",
       " 'live': 636,\n",
       " 'll': 637,\n",
       " 'loathing': 638,\n",
       " 'long': 639,\n",
       " 'longed': 640,\n",
       " 'longer': 641,\n",
       " 'look': 642,\n",
       " 'looked': 643,\n",
       " 'looking': 644,\n",
       " 'lose': 645,\n",
       " 'loss': 646,\n",
       " 'lounging': 647,\n",
       " 'lovely': 648,\n",
       " 'lucky': 649,\n",
       " 'lump': 650,\n",
       " 'luncheon-table': 651,\n",
       " 'luxury': 652,\n",
       " 'lying': 653,\n",
       " 'made': 654,\n",
       " 'make': 655,\n",
       " 'man': 656,\n",
       " 'manage': 657,\n",
       " 'managed': 658,\n",
       " 'mantel-piece': 659,\n",
       " 'marble': 660,\n",
       " 'married': 661,\n",
       " 'may': 662,\n",
       " 'me': 663,\n",
       " 'meant': 664,\n",
       " 'mediocrity': 665,\n",
       " 'medium': 666,\n",
       " 'mentioned': 667,\n",
       " 'mere': 668,\n",
       " 'merely': 669,\n",
       " 'met': 670,\n",
       " 'might': 671,\n",
       " 'mighty': 672,\n",
       " 'millionaire': 673,\n",
       " 'mine': 674,\n",
       " 'minute': 675,\n",
       " 'minutes': 676,\n",
       " 'mirrors': 677,\n",
       " 'modest': 678,\n",
       " 'modesty': 679,\n",
       " 'moment': 680,\n",
       " 'money': 681,\n",
       " 'monumental': 682,\n",
       " 'mood': 683,\n",
       " 'morbidly': 684,\n",
       " 'more': 685,\n",
       " 'most': 686,\n",
       " 'mourn': 687,\n",
       " 'mourned': 688,\n",
       " 'moustache': 689,\n",
       " 'moved': 690,\n",
       " 'much': 691,\n",
       " 'muddling': 692,\n",
       " 'multiplied': 693,\n",
       " 'murmur': 694,\n",
       " 'muscles': 695,\n",
       " 'must': 696,\n",
       " 'my': 697,\n",
       " 'myself': 698,\n",
       " 'mysterious': 699,\n",
       " 'naive': 700,\n",
       " 'near': 701,\n",
       " 'nearly': 702,\n",
       " 'negatived': 703,\n",
       " 'nervous': 704,\n",
       " 'nervousness': 705,\n",
       " 'neutral': 706,\n",
       " 'never': 707,\n",
       " 'next': 708,\n",
       " 'no': 709,\n",
       " 'none': 710,\n",
       " 'not': 711,\n",
       " 'note': 712,\n",
       " 'nothing': 713,\n",
       " 'now': 714,\n",
       " 'nymphs': 715,\n",
       " 'oak': 716,\n",
       " 'obituary': 717,\n",
       " 'object': 718,\n",
       " 'objects': 719,\n",
       " 'occurred': 720,\n",
       " 'oddly': 721,\n",
       " 'of': 722,\n",
       " 'off': 723,\n",
       " 'often': 724,\n",
       " 'oh': 725,\n",
       " 'old': 726,\n",
       " 'on': 727,\n",
       " 'once': 728,\n",
       " 'one': 729,\n",
       " 'ones': 730,\n",
       " 'only': 731,\n",
       " 'onto': 732,\n",
       " 'open': 733,\n",
       " 'or': 734,\n",
       " 'other': 735,\n",
       " 'our': 736,\n",
       " 'ourselves': 737,\n",
       " 'out': 738,\n",
       " 'outline': 739,\n",
       " 'oval': 740,\n",
       " 'over': 741,\n",
       " 'own': 742,\n",
       " 'packed': 743,\n",
       " 'paid': 744,\n",
       " 'paint': 745,\n",
       " 'painted': 746,\n",
       " 'painter': 747,\n",
       " 'painting': 748,\n",
       " 'pale': 749,\n",
       " 'paled': 750,\n",
       " 'palm-trees': 751,\n",
       " 'panel': 752,\n",
       " 'panelling': 753,\n",
       " 'pardonable': 754,\n",
       " 'pardoned': 755,\n",
       " 'part': 756,\n",
       " 'passages': 757,\n",
       " 'passing': 758,\n",
       " 'past': 759,\n",
       " 'pastels': 760,\n",
       " 'pathos': 761,\n",
       " 'patient': 762,\n",
       " 'people': 763,\n",
       " 'perceptible': 764,\n",
       " 'perfect': 765,\n",
       " 'persistence': 766,\n",
       " 'persuasively': 767,\n",
       " 'phrase': 768,\n",
       " 'picture': 769,\n",
       " 'pictures': 770,\n",
       " 'pines': 771,\n",
       " 'pink': 772,\n",
       " 'place': 773,\n",
       " 'placed': 774,\n",
       " 'plain': 775,\n",
       " 'platitudes': 776,\n",
       " 'pleased': 777,\n",
       " 'pockets': 778,\n",
       " 'point': 779,\n",
       " 'poised': 780,\n",
       " 'poor': 781,\n",
       " 'portrait': 782,\n",
       " 'posing': 783,\n",
       " 'possessed': 784,\n",
       " 'poverty': 785,\n",
       " 'predicted': 786,\n",
       " 'preliminary': 787,\n",
       " 'presenting': 788,\n",
       " 'prestidigitation': 789,\n",
       " 'pretty': 790,\n",
       " 'previous': 791,\n",
       " 'price': 792,\n",
       " 'pride': 793,\n",
       " 'princely': 794,\n",
       " 'prism': 795,\n",
       " 'problem': 796,\n",
       " 'proclaiming': 797,\n",
       " 'prodigious': 798,\n",
       " 'profusion': 799,\n",
       " 'protest': 800,\n",
       " 'prove': 801,\n",
       " 'public': 802,\n",
       " 'purblind': 803,\n",
       " 'purely': 804,\n",
       " 'pushed': 805,\n",
       " 'put': 806,\n",
       " 'qualities': 807,\n",
       " 'quality': 808,\n",
       " 'queerly': 809,\n",
       " 'question': 810,\n",
       " 'quickly': 811,\n",
       " 'quietly': 812,\n",
       " 'quite': 813,\n",
       " 'quote': 814,\n",
       " 'rain': 815,\n",
       " 'raised': 816,\n",
       " 'random': 817,\n",
       " 'rather': 818,\n",
       " 're': 819,\n",
       " 'real': 820,\n",
       " 'really': 821,\n",
       " 'reared': 822,\n",
       " 'reason': 823,\n",
       " 'reassurance': 824,\n",
       " 'recovering': 825,\n",
       " 'recreated': 826,\n",
       " 'reflected': 827,\n",
       " 'reflection': 828,\n",
       " 'regrets': 829,\n",
       " 'relatively': 830,\n",
       " 'remained': 831,\n",
       " 'remember': 832,\n",
       " 'reminded': 833,\n",
       " 'repeating': 834,\n",
       " 'represented': 835,\n",
       " 'reproduction': 836,\n",
       " 'resented': 837,\n",
       " 'resolve': 838,\n",
       " 'resources': 839,\n",
       " 'rest': 840,\n",
       " 'rich': 841,\n",
       " 'ridiculous': 842,\n",
       " 'robbed': 843,\n",
       " 'romantic': 844,\n",
       " 'room': 845,\n",
       " 'rose': 846,\n",
       " 'rs': 847,\n",
       " 'rule': 848,\n",
       " 'run': 849,\n",
       " 's': 850,\n",
       " 'said': 851,\n",
       " 'same': 852,\n",
       " 'satisfaction': 853,\n",
       " 'savour': 854,\n",
       " 'saw': 855,\n",
       " 'say': 856,\n",
       " 'saying': 857,\n",
       " 'says': 858,\n",
       " 'scorn': 859,\n",
       " 'scornful': 860,\n",
       " 'secret': 861,\n",
       " 'see': 862,\n",
       " 'seemed': 863,\n",
       " 'seen': 864,\n",
       " 'self-confident': 865,\n",
       " 'send': 866,\n",
       " 'sensation': 867,\n",
       " 'sensitive': 868,\n",
       " 'sent': 869,\n",
       " 'serious': 870,\n",
       " 'set': 871,\n",
       " 'sex': 872,\n",
       " 'shade': 873,\n",
       " 'shaking': 874,\n",
       " 'shall': 875,\n",
       " 'she': 876,\n",
       " 'shirked': 877,\n",
       " 'short': 878,\n",
       " 'should': 879,\n",
       " 'shoulder': 880,\n",
       " 'shoulders': 881,\n",
       " 'show': 882,\n",
       " 'showed': 883,\n",
       " 'showy': 884,\n",
       " 'shrug': 885,\n",
       " 'shrugged': 886,\n",
       " 'sight': 887,\n",
       " 'sign': 888,\n",
       " 'silent': 889,\n",
       " 'silver': 890,\n",
       " 'similar': 891,\n",
       " 'simpleton': 892,\n",
       " 'simplifications': 893,\n",
       " 'simply': 894,\n",
       " 'since': 895,\n",
       " 'single': 896,\n",
       " 'sitter': 897,\n",
       " 'sitters': 898,\n",
       " 'sketch': 899,\n",
       " 'skill': 900,\n",
       " 'slight': 901,\n",
       " 'slightly': 902,\n",
       " 'slowly': 903,\n",
       " 'small': 904,\n",
       " 'smile': 905,\n",
       " 'smiling': 906,\n",
       " 'sneer': 907,\n",
       " 'so': 908,\n",
       " 'solace': 909,\n",
       " 'some': 910,\n",
       " 'somebody': 911,\n",
       " 'something': 912,\n",
       " 'spacious': 913,\n",
       " 'spaniel': 914,\n",
       " 'speaking-tubes': 915,\n",
       " 'speculations': 916,\n",
       " 'spite': 917,\n",
       " 'splash': 918,\n",
       " 'square': 919,\n",
       " 'stairs': 920,\n",
       " 'stammer': 921,\n",
       " 'stand': 922,\n",
       " 'standing': 923,\n",
       " 'started': 924,\n",
       " 'stay': 925,\n",
       " 'still': 926,\n",
       " 'stocked': 927,\n",
       " 'stood': 928,\n",
       " 'stopped': 929,\n",
       " 'stopping': 930,\n",
       " 'straddling': 931,\n",
       " 'straight': 932,\n",
       " 'strain': 933,\n",
       " 'straining': 934,\n",
       " 'strange': 935,\n",
       " 'straw': 936,\n",
       " 'stream': 937,\n",
       " 'stroke': 938,\n",
       " 'strokes': 939,\n",
       " 'strolled': 940,\n",
       " 'strongest': 941,\n",
       " 'strongly': 942,\n",
       " 'struck': 943,\n",
       " 'studio': 944,\n",
       " 'stuff': 945,\n",
       " 'subject': 946,\n",
       " 'substantial': 947,\n",
       " 'suburban': 948,\n",
       " 'such': 949,\n",
       " 'suddenly': 950,\n",
       " 'suffered': 951,\n",
       " 'sugar': 952,\n",
       " 'suggested': 953,\n",
       " 'sunburn': 954,\n",
       " 'sunburnt': 955,\n",
       " 'sunlit': 956,\n",
       " 'superb': 957,\n",
       " 'sure': 958,\n",
       " 'surest': 959,\n",
       " 'surface': 960,\n",
       " 'surprise': 961,\n",
       " 'surprised': 962,\n",
       " 'surrounded': 963,\n",
       " 'suspected': 964,\n",
       " 'sweetly': 965,\n",
       " 'sweetness': 966,\n",
       " 'swelling': 967,\n",
       " 'swept': 968,\n",
       " 'swum': 969,\n",
       " 't': 970,\n",
       " 'table': 971,\n",
       " 'take': 972,\n",
       " 'taken': 973,\n",
       " 'talking': 974,\n",
       " 'tea': 975,\n",
       " 'tears': 976,\n",
       " 'technicalities': 977,\n",
       " 'technique': 978,\n",
       " 'tell': 979,\n",
       " 'tells': 980,\n",
       " 'tempting': 981,\n",
       " 'terra-cotta': 982,\n",
       " 'terrace': 983,\n",
       " 'terraces': 984,\n",
       " 'terribly': 985,\n",
       " 'than': 986,\n",
       " 'that': 987,\n",
       " 'the': 988,\n",
       " 'their': 989,\n",
       " 'them': 990,\n",
       " 'then': 991,\n",
       " 'there': 992,\n",
       " 'therefore': 993,\n",
       " 'they': 994,\n",
       " 'thin': 995,\n",
       " 'thing': 996,\n",
       " 'things': 997,\n",
       " 'think': 998,\n",
       " 'this': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = {word : i for i, word in enumerate(all_words)}\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocabulary):\n",
    "        self.str_to_int = vocabulary\n",
    "        self.int_to_str = {i:word for word,i in vocabulary.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed_text = [x.strip() for x in preprocessed_text if x.strip()]\n",
    "        preprocessed_text = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed_text]\n",
    "        ids = [self.str_to_int[item] for item in preprocessed_text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_text = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokenizer.decode(enc_text[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]  --->> 367\n",
      "[40, 367]  --->> 2885\n",
      "[40, 367, 2885]  --->> 1464\n",
      "[40, 367, 2885, 1464]  --->> 1807\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "for i in range(1,context_size + 1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "    print(context ,\" --->>\" ,desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,text,tokenizer,max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0,len(token_ids) - max_length,stride):\n",
    "            input_chunks = token_ids[i:i+max_length]\n",
    "            target_chunks = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunks))\n",
    "            self.target_ids.append(torch.tensor(target_chunks))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_loaderv1(txt,batch_size=4,max_length = 256,stride = 128,shuffle = True,drop_last = True,num_workers = 0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = shuffle,\n",
    "        num_workers = num_workers,\n",
    "        drop_last=drop_last\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 5 \n",
    "dataloader = create_dataset_loaderv1(\n",
    "    text,\n",
    "    batch_size=8,\n",
    "    max_length=5,\n",
    "    stride=max_length,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "x,y = next(data_iter)\n",
    "len(dataloader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256\n",
    "embeddings_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "token_embeddings = embeddings_layer(x)\n",
    "token_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3333,  0.1342, -0.4348,  ..., -0.4947, -0.2358,  0.7622],\n",
       "        [ 2.1306,  2.3844, -0.5400,  ..., -0.1953, -0.8761,  1.7609],\n",
       "        [-1.8320,  0.9510, -2.8271,  ...,  0.6670, -1.6913, -1.5920],\n",
       "        [ 1.4745, -0.0543, -1.3141,  ..., -0.2592,  1.4522,  0.7471],\n",
       "        [ 0.9863, -1.7034,  0.8604,  ...,  0.6531,  0.1773,  0.6952]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_size = 5\n",
    "poss_embeddings_layer = torch.nn.Embedding(context_size, output_dim)\n",
    "poss_embedding = poss_embeddings_layer(torch.arange(context_size))\n",
    "input_embeddings = token_embeddings + poss_embedding\n",
    "input_embeddings[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple attention mechanism\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43,0.15,0.89],\n",
    "        [0.55, 0.87, 0.66],\n",
    "        [0.57, 0.85, 0.64],\n",
    "        [0.22, 0.58, 0.33],\n",
    "        [0.77, 0.25, 0.10],\n",
    "        [0.05, 0.80, 0.55]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.nn.functional.softmax(attention_scores,dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = attention_weights @ inputs\n",
    "context\n",
    "    \n",
    "tmp_attn = attention_weights[1]\n",
    "con_temp = tmp_attn @ inputs\n",
    "con_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "d_in = inputs.shape[-1]\n",
    "d_out = 2\n",
    "W_query = torch.nn.Parameter(torch.randn(d_in,d_out),requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.randn(d_in,d_out),requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.randn(d_in,d_out),requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key\n",
    "queries = inputs @ W_query\n",
    "values = inputs @ W_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1376)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attention_scores_22 = query_2.dot(keys_2)\n",
    "attention_scores_22 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0740, -0.0216,  0.0126, -0.1230,  0.6250, -0.4498],\n",
       "        [ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809],\n",
       "        [ 0.2098,  0.1320,  0.1665, -0.0489,  0.7408, -0.3725],\n",
       "        [ 0.1458,  0.1061,  0.1254, -0.0118,  0.4384, -0.1919],\n",
       "        [ 0.0175, -0.0071,  0.0017, -0.0321,  0.1580, -0.1153],\n",
       "        [ 0.2240,  0.1642,  0.1935, -0.0161,  0.6667, -0.2888]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_2 = queries @ keys.T\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2833, 0.4180],\n",
       "        [0.2845, 0.4193],\n",
       "        [0.2846, 0.4183],\n",
       "        [0.2861, 0.4026],\n",
       "        [0.2861, 0.3930],\n",
       "        [0.2855, 0.4126]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2,dim=-1)\n",
    "context_2 = attention_weights_2 @ values\n",
    "context_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self,din,dout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(din,dout,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(din,dout,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(din,dout,bias=qkv_bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        keys = self.W_key(x)\n",
    "        queries =self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5,dim=-1)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2846, 0.3766],\n",
       "        [0.2839, 0.3713],\n",
       "        [0.2839, 0.3715],\n",
       "        [0.2853, 0.3782],\n",
       "        [0.2856, 0.3797],\n",
       "        [0.2847, 0.3755]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / inputs.shape[0] **0.5, dim=-1)\n",
    "context_vec = attention_weights @ values\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_mask = torch.tril(torch.ones(attention_scores.shape[0], attention_scores.shape[0]))\n",
    "context_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5299, 0.4701, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3599, 0.3199, 0.3202, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2647, 0.2478, 0.2479, 0.2395, 0.0000, 0.0000],\n",
       "        [0.2100, 0.1991, 0.1991, 0.1935, 0.1983, 0.0000],\n",
       "        [0.1818, 0.1666, 0.1667, 0.1595, 0.1667, 0.1587]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_context = context_mask * attention_weights\n",
    "row_sum = masked_context.sum(dim=-1,keepdim=True)\n",
    "masked_context = masked_context / row_sum\n",
    "masked_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(6,6),diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(),-torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5122, 0.4878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3441, 0.3279, 0.3280, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2560, 0.2491, 0.2492, 0.2457, 0.0000, 0.0000],\n",
       "        [0.2040, 0.1996, 0.1997, 0.1973, 0.1993, 0.0000],\n",
       "        [0.1727, 0.1667, 0.1667, 0.1637, 0.1667, 0.1634]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked/attention_scores.shape[-1],dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 0., 2.],\n",
       "        [2., 0., 0., 2., 0.],\n",
       "        [0., 2., 0., 0., 2.],\n",
       "        [2., 0., 0., 2., 2.],\n",
       "        [0., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(p = 0.5 )\n",
    "dropout(torch.ones(5,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in,d_out,context_len,dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,qkv_bias)\n",
    "        self.droupout = dropout\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_len,context_len),diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, num_tokens,d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        attention_scores = queries @ keys.transpose(1,2)\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens,:num_tokens],\n",
    "            -torch.inf\n",
    "        )        \n",
    "        attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        attention_weights = self.droupout(attention_weights)\n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.stack((inputs,inputs),dim = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2392, -0.7133],\n",
       "        [ 0.1156, -0.3447],\n",
       "        [ 0.2784,  0.4348],\n",
       "        [ 0.2665,  0.1505],\n",
       "        [ 0.3112,  0.2781],\n",
       "        [ 0.0917,  0.1090]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = batch.shape[1]\n",
    "ca = CasualSelfAttention(d_in = 3, d_out=2,context_len=context_len,dropout=dropout)\n",
    "context_vecs = ca(batch)\n",
    "context_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias = False):\n",
    "        super.__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CasualSelfAttention(d_in,d_out,context_len,dropout,qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads()],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(-2, -1)\n",
    "\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens].bool()\n",
    "        attention_scores.masked_fill_(mask_bool, float('-inf'))\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores / self.head_dim ** 0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        return self.out_proj(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(1, emb_dim))  \n",
    "        self.shift = nn.Parameter(torch.zeros(1, emb_dim))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, correction=0) \n",
    "        x_norm = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))* (x + 0.044715 * torch.pow(x,3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_len=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "x = torch.rand(2,4,768)\n",
    "tb = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = tb(x)\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len   = in_idx.shape\n",
    "        tok_embs = self.tok_emb(in_idx)\n",
    "\n",
    "        poss_embs = self.pos_emb(torch.arange(seq_len, device=in_idx.device))  \n",
    "\n",
    "        x = tok_embs + poss_embs\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  105],\n",
       "        [25930]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.softmax(logits[:,-1,:],dim=-1),dim=-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,\n",
    "    max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # idx_cond = idx[:, -context_size:]\n",
    "        # with torch.no_grad():\n",
    "        #     logits = model(idx_cond)\n",
    "        # logits = logits[:, -1, :]\n",
    "        # probas = torch.softmax(logits, dim=-1)\n",
    "        # idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        # idx = torch.cat((idx, idx_next), dim=1)\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next),dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,  2054, 13875, 45984, 37088, 33683,  9791]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am tre tangOnt requisite acquisitions beings'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out[0].tolist())\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:green\">TRAINING CHAPTER BEGIN</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,\n",
    "\"context_length\": 256,\n",
    "\"emb_dim\": 768,\n",
    "\"n_heads\": 12,\n",
    "\"n_layers\": 12,\n",
    "\"drop_rate\": 0.1,\n",
    "\"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(ids,tokenizer):\n",
    "    flat = ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(start_context, tokenizer),\n",
    "max_new_tokens=10,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "[40, 1107, 588]])\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "[1107, 588, 11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Armed heNetflix pressuring empoweredfaith'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "decoded_text = tokenizer.decode(token_ids.flatten().tolist())\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "print(len(tokenizer.encode(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio*len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "len(train_data)\n",
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataset_loaderv1(\n",
    "    txt=train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = create_dataset_loaderv1(\n",
    "    txt=val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader,model,device,num_batches= None):\n",
    "    total_loss = 0\n",
    "    if(len(data_loader) == 0):\n",
    "        return float(\"nan\")\n",
    "\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "\n",
    "    else:\n",
    "        num_batches = min(num_batches,len(data_loader))\n",
    "\n",
    "    for i,(x,y) in enumerate(data_loader):\n",
    "        if(i < num_batches):\n",
    "            loss = calc_loss_batch(x,y,model=model,device=device)\n",
    "            total_loss+=loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "    model=model, idx=encoded,\n",
    "    max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "optimizer, device, num_epochs,\n",
    "eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                f\"Train loss {train_loss:.3f}, \"\n",
    "                f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "            generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "            )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the the,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you,.                                                \n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you,,,, the the the the the the,,,, the the the,, the the,,,, the the the, the the the the the the the the the the the, the the the the the the,,,\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you, the, the the the the the the the the the the the the the the the the the the the the the                          \n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Every effort moves you,.                                                \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you, the, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you, and, and, the, and,,,, and, and, and, the to the, the, and, and,, the, and, the to the, and, and, the, and,, and,, and\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and,, and, and,,, and, and, and,, and,,,, and, and,, and,,,, and,, and,,, and, and,,, and, and\n",
      "Every effort moves you, and, and, and, and,,,, and, and, and,, and,,,, and, and,, and, and,, and,, and,,, and, and,,, and, and\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Every effort moves you, and, and the. \"\"\", and the. \"\", and the. \"\"  \"\". \"\"\"\"\"\".   \"\"\"\"\"\"\"\"\"\"\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Every effort moves you, and I had\"                                             \n",
      "Every effort moves youNESSNESSNESSNESSNESSNESS Scouts THCNESSNESSNESSNESSNESSNESSNESS Scouts THC 390 Scouts 390 THC accessibility THC THC THC THC accessibility THC THC THC\n",
      "Every effort moves you, and I had to the, and I had.                                       \n",
      "Every effort moves you. \"I the the of the his--I--I the of the the of the--I--I.  \"I. I had the the the of the the of the of the the of the--I--I was the\n",
      "Every effort moves you his! Gisburn. Gisburn to the picture. Gisburn. Gisburn's--III had been to the the his his the hisisburn. Gisburn to the _ a--I to Gisburn\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you his! Gisburn. Gisburn to the picture.   \"I        \"I. \"I the of the--and it.  \"I was--III was the\n",
      "Every effort moves you, and I had been.                                            \n",
      "Every effort moves you, and I had been.                                            \n",
      "Every effort moves you, and I had been the picture.                                          \n",
      "Every effort moves you know\"I\"I was a\"I was a\"I looked.           \"I was the picture and I had\"I had the the picture to the picture and I was.  \"\n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Every effort moves you, and I had been the picture. \"I was aI was a\", I had been--and, I had been to the picture. I had the picture. I had the picture to the picture. \"I he was a\n",
      "Every effort moves you, and I had been the picture--I had been--I had been, I had been--I had been, I had been, I had been, I had been, and I had been, I had been--and, I had been\n",
      "Every effort moves you know                                                 \n",
      "Every effort moves you know                                                 \n",
      "Every effort moves you know the                                                \n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Every effort moves you know the  \"I had been the picture--I had been.          \"Oh, and I felt--as Jack he had been the and I had been, and down, and he had been\n",
      "Every effort moves you know the \"I had been--I had been the donkey. \"Oh, I had been--and here are the donkey to the donkey, I had been to have. \"I had been, and down the room, I had\n",
      "Every effort moves you know the      \"--I had been his last I had been--and by me to see.   \"I had been the donkey to me.   \"I had been the fact--and it's\n",
      "Every effort moves you know                        He laughed again, and he had the donkey and he had been his pictures he had been the donkey, and he was his\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know                        He laughed again, and he had the donkey and he had been his pictures of his pictures and down the room, and he\n",
      "Every effort moves you know it was his a little of his pictures--I turned.            \"Oh, and he was his pictures--as at my elbow and I felt of his pictures--because he was his pictures\n",
      "Every effort moves you know the fact of his pictures of his pictures--I had been the last I felt to have to see it the fact, and to see.       \"Oh, and he was, and down the room, and of\n",
      "Every effort moves you know the fact with a little the of the--I had been the last I.        \"Oh, and the fact--as Jack's the donkey.  \"I up and down the room, and in\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Every effort moves you know it was not that the picture--I had the fact with the last--his, and I was--and here are the Riv, and I had been his head to have.             \n",
      "Every effort moves you know it was not that, and in the--I had a little of a little: \"--his, the fact, and in a little.\"    \"I.  \"Oh, and in the donkey. \"I\n",
      "Every effort moves you know,\" was not that, and in the to the fact of the last--his, and--I turned to me, and in the of the, in the picture to me.  \"Oh, and in the donkey, and in the\n",
      "Every effort moves you know,\" was one of the picture, and he, and Mrs. \"Oh, my work, and in fact, and to see the end of the his head to me. \"I didn't, and down the room, my dear\n",
      "Every effort moves you know,\" was one of the picture.  I turned to the last word.     \"I didn't you know, I, my dear, my, my elbow and I had the donkey. \"I didn't you\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture.  I turned to the last word.        He laughed again, I had back his head to the donkey.  \"I didn't--the. \"I\n",
      "Every effort moves you know,\" was not that I had been his pictures--the a little of a. Gisburn's an!         He placed them at my elbow and I had the donkey. \"There were, I had\n",
      "Every effort moves you know,\" was one of the picture. Gisburn--as his last I had been his eyes's an!             \"--and it, I had the--the.   \n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Every effort moves you know,\" was one of the picture. The mere, and I had been I had been his eyes's past!         He placed them at my elbow and as his pictures, and down the room, and in\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Every effort moves you know,\" was not that I felt as it was the fact with a little a. Gisburn's past!     \"I was back the head to the donkey.  \"I looked up--because he had always his\n",
      "Every effort moves you know,\" was one of the picture for a smile that I had the last word. Gisburn's past!  \"I didn't say, and I had been_ as his own the his pictures--and--because he had always his\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no I was the fact, and that, and I was back his head to look up at the honour of the donkey, and, and he was his\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no I felt to me to have to see a little, and I had been_ as his own his own the donkey, and, and he was his\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little a flash that he _rose of the fact, the cigars you like.\"  He placed them at my elbow and as I had been the--because he had always _\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little a flash that he _rose of the fact, the cigars you like.\"  He placed them at my elbow and as I had been the--because he had always _\n",
      "Every effort moves you?\" \"I that the picture for a--I told Mrs. \"Oh, the--I looked up, I felt to see a little remember getting off a prodigious phrase about the honour of the donkey, and were, and in his\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no great, one, I felt to see a smile behind his pictures.  \"Oh, with him, with a--because he's an his\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no great surprise to me to have to see a smile behind his pictures.  \"Oh, I saw that, and down, and I was.\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no great surprise to me to have to see a smile behind his pictures.  \"Oh, I saw that, and down, and I was.\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \" to my work, and the fact, and I felt a smile behind his close grayish beard--as if he had the, I had been. \"I\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \" to my work, and the fact, and to see a smile behind his close grayish beard--as if he had the donkey.  \"I was.\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no great surprise to me to have to see a smile behind his close grayish beard--as if he had the donkey.      \n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Every effort moves you know,\" was one of the axioms he, so inevitably the last word. Gisburn's an awful simpleton, had been the display of his pictures--as, as once one had longed to say: \"Be dissatisfied with your\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m      6\u001b[0m lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 9\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 14\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(\n\u001b[1;32m     11\u001b[0m input_batch, target_batch, model, device\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m tokens_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     16\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/gpt/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/gpt/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/gpt/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpt/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpt/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:426\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    425\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[0;32m--> 426\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    429\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "model.parameters(),\n",
    "lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " It had always been his fate to have womened frame. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"It had always been his fate to have women\", tokenizer),\n",
    "max_new_tokens=25,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Amaan is a good boy\"\n",
    "ids = torch.tensor(tokenizer.encode(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myemb = nn.Embedding(vocab_size,5,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8858,  0.5565, -0.2721,  1.1316,  0.4890],\n",
       "        [ 0.9762,  0.4758, -0.2119, -0.8834, -2.3604],\n",
       "        [-1.1884,  0.1559,  1.0900, -0.0044,  0.4302],\n",
       "        [ 1.8207,  0.3162,  0.6851, -0.0147,  0.1794],\n",
       "        [-0.3821,  0.1976, -0.6816,  0.0609,  0.9262],\n",
       "        [ 0.4095,  0.2131, -1.9820, -1.3616,  1.9372],\n",
       "        [ 1.5138,  1.0505, -1.2629, -0.1244, -0.5657]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = myemb(ids)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
